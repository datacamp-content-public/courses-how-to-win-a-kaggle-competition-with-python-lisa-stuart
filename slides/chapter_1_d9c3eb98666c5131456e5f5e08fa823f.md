---
title: Insert title here
key: d9c3eb98666c5131456e5f5e08fa823f

---
## Introduction to Decision Trees in Python

```yaml
type: "TitleSlide"
key: "ff781fae0e"
```

`@lower_third`

name: Lisa Stuart
title: Data Scientist


`@script`
Previously, you created a predictive modeling template.  Let’s apply it to the Decision Tree algorithm, a non-parametric supervised learning method that creates a series of binary splits on the features in a dataset.


---
## Regression Tree

```yaml
type: "FullCodeSlide"
key: "d792898f4f"
center_content: true
```

`@part1`
Boston House Prices Dataset

```
boston = load_boston()
X = boston['data']
y = boston['target']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=123)
print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)```


`@script`
We’ll start out building a regression tree using the Boston housing dataset.  After importing it along with any other necessary libraries, this code defines the X matrix and y variable, which we can then input into the train test split function and we’re ready to proceed with model training.


---
## Regression Tree

```yaml
type: "FullCodeSlide"
key: "9b0dad66c1"
center_content: true
```

`@part1`
```
# Step 1. Import
from sklearn.tree import DecisionTreeRegressor 

# Step 2. Instantiate
reg_tree = DecisionTreeRegressor(random_state = 100, max_depth=3,  min_samples_leaf=5) 

# Step 3. Fit
reg_model = reg_tree.fit(X_train, y_train)

# Step 4. Predict
pred = reg_tree.predict(X_test)
```


`@script`
Step 1 imports the DecisionTreeRegressor machine learning function and step 2 instantiates.  Although several arguments can be passed, we’ll use random state for reproducibility and max depth of 3, which is how deep to grow the tree.  Min_samples_leaf sets the minimum observations required for a terminal leaf node.  Step 3 fits the instantiation from step 2 by calling dot fit and using x_train and y_train.  Step 4 calls dot predict using x test.


---
## Regression Tree Visualization

```yaml
type: "FullCodeSlide"
key: "ef5347a249"
center_content: true
```

`@part1`
```
from IPython.display import Image 
from sklearn.tree import export_graphviz
import pydotplus

# Create DOT data
dot_data = tree.export_graphviz(reg_tree, out_file=None,
                                filled=True, rounded=True,
                                feature_names=boston.feature_names,
                                max_depth = 3)
# Draw graph
graph = pydotplus.graph_from_dot_data(dot_data)  

# Show graph
Image(graph.create_png())
```


`@script`
Since this is a tree, let’s create a visualization to help make a bit more sense out of what is happening here.  We’ll import packages and create the dot_data, passing arguments that result in a better-looking tree representation, including feature names to help with interpretation.


---
## Regression Tree Visualization

```yaml
type: "FullImageSlide"
key: "df87f722b7"
center_content: true
```

`@part1`
![Boston Tree](https://raw.githubusercontent.com/LisaStuart5678/Datasets/master/boston.png)


`@script`
The tree has 3 levels, the top root node being the first split on the data.  Here, splitting on 7.092 rooms reduced the mean squared error the most compared to other feature and value split combinations, partitioning to the left child node the observations that evaluated to true, with the remainder sent to the right child node.  Mse of the observations is given as 85.787.  Samples is equivalent to the number of observations and value is the mean of the values for the target variable.  The left child node splits on the lstat variable for the value 14.905 with its left child node also splitting on lstat of less than or equal to 4.915, taking us to the leaf or terminal nodes with true observations left and false right.  The trained decision tree model is a set of decision rules inferred from the training observations.  For example, following the appropriate branches until reaching a leaf node, a home in Boston with 4 bedrooms and lstat 6 is worth $22,812.


---
## Classification Tree

```yaml
type: "FullCodeSlide"
key: "eac5135d81"
center_content: true
```

`@part1`
```
names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']
diabetes = pd.read_csv(r'diabetes.csv', names=names)
class_names = ['neg','pos']
diabetes = diabetes.values
X = diabetes[:, 0:8]
y = diabetes[:,8]

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=123)
print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)

# Step 1. Import
from sklearn.tree import DecisionTreeClassifier

# Step 2. Instantiate
cl_tree = DecisionTreeClassifier(random_state=567, max_depth=3, min_samples_leaf=5) 

# Step 3. Fit
cl_model = cl_tree.fit(X_train, y_train) 

 # Step 4. Predict
pre = cl_tree.predict(X_test)
```


`@script`
If we wanted to build a classification tree, we would instead use the DecisionTreeClassifier machine learning function.  Here is example code using the Diabetes dataset.


---
## Classification Tree Visualization

```yaml
type: "FullCodeSlide"
key: "9e03c492cf"
center_content: true
```

`@part1`
```
# visualize
from IPython.display import Image 
from sklearn.tree import export_graphviz
import pydotplus

# Create DOT data
dot_data = tree.export_graphviz(cl_tree, out_file=None,
                                filled=True, rounded=True,
                                special_characters=True,
                                feature_names=names[:8],
                                class_names=class_names)
# Draw graph
graph = pydotplus.graph_from_dot_data(dot_data)  

# Show graph
Image(graph.create_png())
```


`@script`
And here the code to create the tree visualization.


---
## Classification Tree Visualization

```yaml
type: "FullImageSlide"
key: "c2f8220c62"
center_content: true
```

`@part1`
![Diabetes Tree](https://raw.githubusercontent.com/LisaStuart5678/Datasets/master/diabetes.png)


`@script`
The root node splits on plasma glucose levels at 154.5.  The default splitting criterion is the gini index, a measure of node purity with 0 indicating perfect purity.  The leaf nodes use the majority class as the final prediction.  Note how second level splits predict the same class, likely a result of imbalanced classes and tree depth.


---
## Decision Trees Are:

```yaml
type: "FullSlide"
key: "29b1205e82"
```

`@part1`
- greedy

- choose best local optimum split


- not usually best overall tree


`@script`
Decision trees are considered ‘greedy’ algorithms in that they make the best locally optimal decision which may not be the overall optimal tree.  We’ll improve upon this in upcoming lessons.


---
## Summary:

```yaml
type: "FullSlide"
key: "26266276d2"
center_content: false
disable_transition: true
```

`@part1`
DecisionTreeRegressor - mse

DecisionTreeClassifier - gini


![def1](https://raw.githubusercontent.com/LisaStuart5678/Datasets/master/definition1.png)


`@script`
To summarize, the two different functions for Decision Trees are the DecisionTreeRegressor using mse as a measure of split quality and the DecisionTreeClassifier using the gini index.  We defined the root node as the first split,


---
## Summary:

```yaml
type: "FullSlide"
key: "909248b6c7"
disable_transition: true
```

`@part1`
DecisionTreeRegressor - mse

DecisionTreeClassifier - gini


![def2](https://raw.githubusercontent.com/LisaStuart5678/Datasets/master/definition2.png)


`@script`
the child nodes as the left and right branched nodes


---
## Summary:

```yaml
type: "FullSlide"
key: "0240a76026"
disable_transition: true
```

`@part1`
DecisionTreeRegressor - mse

DecisionTreeClassifier - gini


![def3](https://raw.githubusercontent.com/LisaStuart5678/Datasets/master/definition3.png)


`@script`
coming from their respective parent nodes


---
## Summary:

```yaml
type: "FullSlide"
key: "48145178c2"
disable_transition: true
```

`@part1`
DecisionTreeRegressor - mse

DecisionTreeClassifier - gini


![def4](https://raw.githubusercontent.com/LisaStuart5678/Datasets/master/definition4.png)


`@script`
and the leaf nodes which terminate in the final predictions for the trained model being a continuous value in the case of regression or the majority class for a classifier.


---
## Your turn to practice building a Decision Tree in Python!

```yaml
type: "FinalSlide"
key: "d41ce044bc"
```

`@script`
Now, it’s your turn to go and practice building your first decision tree model in Python.

