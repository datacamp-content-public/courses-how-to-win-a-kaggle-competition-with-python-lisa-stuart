---
title: Insert title here
key: d9c3eb98666c5131456e5f5e08fa823f

---
## Introduction to Decision Trees in Python

```yaml
type: "TitleSlide"
key: "ff781fae0e"
```

`@lower_third`

name: Lisa Stuart
title: Data Scientist


`@script`
Previously, you created a predictive modeling template.  Let’s apply it to the Decision Tree algorithm.


---
## Regression Tree

```yaml
type: "FullCodeSlide"
key: "d792898f4f"
center_content: true
```

`@part1`
Boston House Prices Dataset

```
boston = load_boston()
X = boston['data']
y = boston['target']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=123)
print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)```


`@script`
We’ll start with a regression tree using the Boston housing dataset as the code demonstrates.


---
## Regression Tree

```yaml
type: "FullCodeSlide"
key: "9b0dad66c1"
center_content: true
```

`@part1`
```
# Step 1. Import
from sklearn.tree import DecisionTreeRegressor 

# Step 2. Instantiate
reg_tree = DecisionTreeRegressor(random_state = 100, max_depth=3,  min_samples_leaf=5) 

# Step 3. Fit
reg_model = reg_tree.fit(X_train, y_train)

# Step 4. Predict
pred = reg_tree.predict(X_test)
```


`@script`
Step 1 imports the DecisionTreeRegressor, step 2 instantiates.  Although several arguments can be passed, we’ll use random state for reproducibility and max depth of 3, which is how deep to grow the tree.  Min_samples_leaf sets the minimum observations required for a terminal leaf node.
Step 3 calls dot fit on the instantiated model using x_train and y_train.  Step 4 calls dot predict using x test.


---
## Regression Tree Visualization

```yaml
type: "FullCodeSlide"
key: "ef5347a249"
center_content: true
```

`@part1`
```
from IPython.display import Image 
from sklearn.tree import export_graphviz
import pydotplus

# Create DOT data
dot_data = tree.export_graphviz(reg_tree, out_file=None,
                                filled=True, rounded=True,
                                feature_names=boston.feature_names,
                                max_depth = 3)
# Draw graph
graph = pydotplus.graph_from_dot_data(dot_data)  

# Show graph
Image(graph.create_png())
```


`@script`
Visualization will make a bit more sense out of the results. We’ll import packages, create the dot data, passing some arguments that result in a better-looking tree representation.  Passing our feature names helps with interpretation.


---
## Regression Tree Visualization

```yaml
type: "FullImageSlide"
key: "df87f722b7"
center_content: true
```

`@part1`
![Boston Tree](https://raw.githubusercontent.com/LisaStuart5678/Datasets/master/boston.png)


`@script`
The tree has 3 levels, the top root node being the first split on the data.  Here, splitting on 7.092 rooms reduced the mean squared error the most compared to other feature and value split combinations, partitioning to the left child node the observations that evaluated to true, with the remainder sent to the right child node.  Samples is equivalent to the number of observations and value is the average target value.  The left child node has a split on the lstat variable for the value 14.905 with that node’s left child node again splitting on lstat of less than or equal to 4.915, taking us to the leaf or terminal node with true observations left and false right.  The trained decision tree model is a set of decision rules inferred from the training observations.  For example, following the appropriate branches until reaching a leaf node, a home in Boston that has 4 bedrooms and an lstat value of 6 is worth $22,812.


---
## Classification Tree

```yaml
type: "FullCodeSlide"
key: "eac5135d81"
center_content: true
```

`@part1`
```
names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']
diabetes = pd.read_csv(r'diabetes.csv', names=names)
class_names = ['neg','pos']
diabetes = diabetes.values
X = diabetes[:, 0:8]
y = diabetes[:,8]

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=123)
print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)
```


`@script`
Now we’ll build a classification tree with the Diabetes dataset.


---
## Classification Tree

```yaml
type: "FullCodeSlide"
key: "f520629978"
center_content: true
```

`@part1`
```
# Step 1. Import
from sklearn.tree import DecisionTreeClassifier

# Step 2. Instantiate
cl_tree = DecisionTreeClassifier(random_state=567, max_depth=3, min_samples_leaf=5) 

# Step 3. Fit
cl_model = cl_tree.fit(X_train, y_train) 

 # Step 4. Predict
pre = cl_tree.predict(X_test)
```


`@script`
We import the DecisionTreeClassifier machine learning function, instantiate, fit and predict.


---
## Classification Tree Visualization

```yaml
type: "FullCodeSlide"
key: "9e03c492cf"
center_content: true
```

`@part1`
```
# visualize
from IPython.display import Image 
from sklearn.tree import export_graphviz
import pydotplus

# Create DOT data
dot_data = tree.export_graphviz(cl_tree, out_file=None,
                                filled=True, rounded=True,
                                special_characters=True,
                                feature_names=names[:8],
                                class_names=class_names)
# Draw graph
graph = pydotplus.graph_from_dot_data(dot_data)  

# Show graph
Image(graph.create_png())
```


`@script`
The visualization code


---
## Classification Tree Visualization

```yaml
type: "FullImageSlide"
key: "c2f8220c62"
center_content: true
```

`@part1`
![Diabetes Tree](https://raw.githubusercontent.com/LisaStuart5678/Datasets/master/diabetes.png)


`@script`
The tree shows the root node splits on plasma glucose levels with a split value of 154.5.  The default splitting criterion is the gini index, a measure of node purity with 0 indicating perfect purity.  With 537 samples, 357 have the 0 or negative class while 180 are class 1, positive for diabetes, a 2 to 1 class imbalance ratio.  A plas greater than 154.5 would evaluate to false following the right branch.  The right child node also shows plas with a split of 166.5, containing 19 negative and 62 positive observations.  A 180 plas follows the right branch and an age of 50 the left branch, arriving at a terminal leaf node with the class majority becoming the class prediction.  Note how second level splits predict the same class. This is likely a result of imbalanced classes as well as the depth we grew the tree to.


---
## Decision Trees Are:

```yaml
type: "FullSlide"
key: "29b1205e82"
```

`@part1`
- greedy

- choose best local optimum split


- not usually best overall tree


`@script`
Decision trees are considered ‘greedy’ algorithms in that they make the best locally optimal decision which may not be the overall optimal tree.  We’ll improve upon this in upcoming lessons.


---
## Summary:

```yaml
type: "FullSlide"
key: "26266276d2"
center_content: false
```

`@part1`
DecisionTreeRegressor - mse

DecisionTreeClassifier - gini


![def1](https://raw.githubusercontent.com/LisaStuart5678/Datasets/master/definition1.png)


`@script`
To summarize, the two different functions for Decision Trees are the Decision Tree Regressor using mse as a measure of split quality and the Decision Tree Classifier using the gini index.  We defined the root node as the first split,


---
## Summary:

```yaml
type: "FullSlide"
key: "909248b6c7"
```

`@part1`
DecisionTreeRegressor - mse

DecisionTreeClassifier - gini


![def2](https://raw.githubusercontent.com/LisaStuart5678/Datasets/master/definition2.png)


`@script`
the child nodes as the left and right branched nodes


---
## Summary:

```yaml
type: "FullSlide"
key: "0240a76026"
```

`@part1`
DecisionTreeRegressor - mse

DecisionTreeClassifier - gini


![def3](https://raw.githubusercontent.com/LisaStuart5678/Datasets/master/definition3.png)


`@script`
coming from their respective parent nodes


---
## Summary:

```yaml
type: "FullSlide"
key: "48145178c2"
```

`@part1`
DecisionTreeRegressor - mse

DecisionTreeClassifier - gini


![def4](https://raw.githubusercontent.com/LisaStuart5678/Datasets/master/definition4.png)


`@script`
and the leaf nodes which terminate in the final predictions for the trained model being a continuous value in the case of regression or the majority class for a classifier.


---
## Your turn to build a Decision Tree in Python!

```yaml
type: "FinalSlide"
key: "d41ce044bc"
```

`@script`
Now, it’s your turn to go and practice building your first decision tree model in Python.

